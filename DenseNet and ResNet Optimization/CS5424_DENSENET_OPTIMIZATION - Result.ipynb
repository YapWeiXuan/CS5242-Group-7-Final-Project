{"cells":[{"cell_type":"code","execution_count":null,"id":"03dbfd15-40fc-45c1-b288-d931c3c576b9","metadata":{"id":"03dbfd15-40fc-45c1-b288-d931c3c576b9","tags":[]},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from datetime import datetime\n","\n","import PIL\n","# Rmb to run \"pip install pillow\"\n","# from ResNet import Bottleneck, ResNet, ResNet50"]},{"cell_type":"code","execution_count":null,"id":"a9547f4d-8e25-421d-9348-8585fc3fc91b","metadata":{"id":"a9547f4d-8e25-421d-9348-8585fc3fc91b","tags":[]},"outputs":[],"source":["import os\n","from os import listdir"]},{"cell_type":"code","execution_count":null,"id":"5324cac0-d4d4-4738-bea2-7ff0080798c7","metadata":{"id":"5324cac0-d4d4-4738-bea2-7ff0080798c7","tags":[]},"outputs":[],"source":["# Folders\n","train_FAKE_folder_path = \"C:/Users/tan_l/Workspace/CS5242/Group Project/Data/Actual Subset/train/fake\"\n","train_REAL_folder_path = \"C:/Users/tan_l/Workspace/CS5242/Group Project/Data/Actual Subset/train/real\"\n","\n","test_FAKE_folder_path = \"C:/Users/tan_l/Workspace/CS5242/Group Project/Data/Actual Subset/test/fake\"\n","test_REAL_folder_path = \"C:/Users/tan_l/Workspace/CS5242/Group Project/Data/Actual Subset/test/real\""]},{"cell_type":"markdown","id":"87c797b9-4410-410b-b94e-cf3be1f4dd6b","metadata":{"id":"87c797b9-4410-410b-b94e-cf3be1f4dd6b"},"source":["## Prepare TRAIN and TEST Tensor dataset (Let label for Real be 1 and label for Fake be 0)"]},{"cell_type":"code","execution_count":null,"id":"f0c6899c-c9db-42dc-a9e0-1065ae444c93","metadata":{"id":"f0c6899c-c9db-42dc-a9e0-1065ae444c93","tags":[]},"outputs":[],"source":["def ignore_alpha(image):\n","    if image.mode == 'RGBA':\n","        r, g, b, _ = image.split()\n","        return PIL.Image.merge(\"RGB\", (r, g, b))\n","    elif image.mode == 'RGB':\n","        return image\n","    else:\n","        return image.convert(\"RGB\")\n","\n","transform_train_V1 = transforms.Compose([\n","    ignore_alpha,\n","    transforms.Resize((224,224)),\n","    transforms.CenterCrop((224,224)),\n","    transforms.RandomHorizontalFlip(), # To introduce image orientation variation\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","])\n","\n","transform_test_V1 = transforms.Compose([\n","    ignore_alpha,\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])"]},{"cell_type":"code","execution_count":null,"id":"2d5e446e-6c59-448c-a7a5-981331907437","metadata":{"id":"2d5e446e-6c59-448c-a7a5-981331907437","tags":[]},"outputs":[],"source":["train_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(train_FAKE_folder_path):\n","    # print(images)\n","    # if images.endswith(\".png\"):\n","    img = PIL.Image.open(f\"{train_FAKE_folder_path}/%s\" % images)\n","    # train_tensor_list.append(transform_train_V1(img))\n","    train_tensor_list.append((transform_train_V1(img), 0))\n","# REAL portion\n","for images in os.listdir(train_REAL_folder_path):\n","    # if images.endswith(\".png\"):\n","    img = PIL.Image.open(f\"{train_REAL_folder_path}/%s\" % images)\n","    # train_tensor_list.append(transform_train_V1(img))\n","    train_tensor_list.append((transform_train_V1(img), 1))"]},{"cell_type":"code","execution_count":null,"id":"3ed01871-79f8-4362-857e-d3484c27c429","metadata":{"id":"3ed01871-79f8-4362-857e-d3484c27c429","tags":[]},"outputs":[],"source":["test_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(test_FAKE_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{test_FAKE_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    test_tensor_list.append((transform_test_V1(img),0))\n","# REAL portion\n","\n","for images in os.listdir(test_REAL_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{test_REAL_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    test_tensor_list.append((transform_test_V1(img),1))"]},{"cell_type":"code","execution_count":null,"id":"c2eb2395-5cc2-46f1-a1c3-ae852b4b634c","metadata":{"id":"c2eb2395-5cc2-46f1-a1c3-ae852b4b634c","outputId":"5c350bcc-253e-43cf-804d-1f2629e22df7"},"outputs":[{"name":"stdout","output_type":"stream","text":["3300\n","1200\n"]}],"source":["print(len(train_tensor_list))\n","print(len(test_tensor_list))"]},{"cell_type":"code","execution_count":null,"id":"e8c2866e-cd19-4145-8683-f2d90bb31d65","metadata":{"id":"e8c2866e-cd19-4145-8683-f2d90bb31d65","outputId":"17eb2789-3248-4425-caf3-db8ba9a4ed77"},"outputs":[{"name":"stdout","output_type":"stream","text":["3300\n","1200\n"]}],"source":["print(len(train_tensor_list))\n","print(len(test_tensor_list))"]},{"cell_type":"code","execution_count":null,"id":"57bbfdd2-de16-4f98-a203-b58378525dcb","metadata":{"id":"57bbfdd2-de16-4f98-a203-b58378525dcb","outputId":"f813eb1f-d447-4611-afed-321bcb47d63e","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, H, W]: torch.Size([128, 3, 224, 224])\n","Shape of y: torch.Size([128]) torch.int64\n"]}],"source":["## LOAD DATA to trainloader and testloader\n","\n","# train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","\n","trainloader = torch.utils.data.DataLoader(train_tensor_list, batch_size=128, shuffle=True, num_workers=2)\n","\n","# test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","testloader = torch.utils.data.DataLoader(test_tensor_list, batch_size=128,shuffle=False, num_workers=2)\n","\n","for X, y in trainloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","id":"8d8c82fa-8cdc-43ad-985f-a57c03f62ffb","metadata":{"id":"8d8c82fa-8cdc-43ad-985f-a57c03f62ffb"},"source":["### DenseNET Model 121"]},{"cell_type":"code","execution_count":null,"id":"cd115c53-996a-464f-82a8-19390dd3b41e","metadata":{"id":"cd115c53-996a-464f-82a8-19390dd3b41e","outputId":"4d403691-fd87-48a0-ebc6-8c1516394d9d","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n","C:\\Users\\tan_l\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","C:\\Users\\tan_l\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","\n","# input_batch = torch.stack(train_tensor_list)\n","# print(input_batch)\n","\n","# device = (\n","#     \"cuda\"\n","#     if torch.cuda.is_available()\n","#     else \"mps\"\n","#     if torch.backends.mps.is_available()\n","#     else \"cpu\"\n","# )\n","\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f\"Using {device} device\")\n","\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","# # or any of these variants\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)\n","# model.eval()\n","\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n","# or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n","model.eval()\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","\n","# with torch.no_grad():\n","#     output = model(input_batch)\n","# # Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n","# print(output[0])\n","# # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n","# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n","# print(probabilities)"]},{"cell_type":"code","execution_count":null,"id":"11e4af2b-9435-46a3-8b5b-4d23433ee7a7","metadata":{"id":"11e4af2b-9435-46a3-8b5b-4d23433ee7a7","outputId":"c983d87b-cbeb-4a42-bb5f-c5ef4c3fb9ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-28 22:37:36.115947\n","Learning Rate: 0.01\n","Start Time : 1714315056.1159477\n","End Time : 1714320164.8517566\n","Time Taken: 5108.735808849335\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 0.3399777770042419\n","Test Accuracy : 85.91666666666666\n","\n","-------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-29 00:02:45.085735\n","Learning Rate: 0.001\n","Start Time : 1714320165.0857353\n","End Time : 1714325247.4073293\n","Time Taken: 5082.321593999863\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 0.49918232560157777\n","Test Accuracy : 78.08333333333334\n","\n","-------------------------------\n","Started: 2024-04-29 01:27:27.578346\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Learning Rate: 0.0001\n","Start Time : 1714325247.578347\n","End Time : 1714330320.2631247\n","Time Taken: 5072.684777736664\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(6.8101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 7.172862195968628\n","Test Accuracy : 3.0833333333333335\n","\n","-------------------------------\n","Done!\n"]}],"source":["import time\n","\n","optimizer_lr =[1e-2,1e-3,1e-4]\n","\n","for j in optimizer_lr:\n","    model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","    model.train()\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=j)\n","\n","    def train(dataloader, model, loss_fn, optimizer):\n","        size = len(dataloader.dataset)\n","        model.train()\n","        for batch, (X, y) in enumerate(dataloader):\n","            X, y = X.to(device), y.to(device)\n","\n","            # Compute prediction error\n","            pred = model(X)\n","            loss = loss_fn(pred, y)\n","\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            if batch % 100 == 0:\n","                loss, current = loss.item(), (batch + 1) * len(X)\n","                # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","        return loss\n","\n","    def test(dataloader, model, loss_fn):\n","        size = len(dataloader.dataset)\n","        num_batches = len(dataloader)\n","        model.eval()\n","        test_loss, correct = 0, 0\n","        with torch.no_grad():\n","            for X, y in dataloader:\n","                X, y = X.to(device), y.to(device)\n","                pred = model(X)\n","                test_loss += loss_fn(pred, y).item()\n","                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","        test_loss /= num_batches\n","        correct /= size\n","        # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","        return (100*correct) , test_loss\n","\n","    epochs = 10\n","    print(\"Started:\", datetime.now())\n","\n","    start = time.time()\n","\n","    for t in range(epochs):\n","\n","        loss = train(trainloader, model, loss_fn, optimizer)\n","        accuracy , avg_loss = test(testloader, model, loss_fn)\n","        if t == (epochs-1):\n","            end = time.time()\n","            print(\"Learning Rate:\", j)\n","            print(\"Start Time :\", start)\n","            print(\"End Time :\", end)\n","            print(\"Time Taken:\", end - start)\n","            print(f\"Epoch {t+1}\\n-------------------------------\")\n","            print(\"Train Loss :\", loss)\n","            print(\"Test Avg Loss :\", avg_loss)\n","            print(\"Test Accuracy :\", accuracy)\n","            print(\"\\n-------------------------------\")\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"id":"437070ed-67a6-48c1-8248-5a310ab935b1","metadata":{"id":"437070ed-67a6-48c1-8248-5a310ab935b1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"31af7e90-105c-48e3-9ae8-bec35297ac9c","metadata":{"id":"31af7e90-105c-48e3-9ae8-bec35297ac9c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"45d9c241-5cbf-40a9-8fd6-4eaf6d41888e","metadata":{"id":"45d9c241-5cbf-40a9-8fd6-4eaf6d41888e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0bbd207e-8294-4889-9514-a9cf183acbd4","metadata":{"id":"0bbd207e-8294-4889-9514-a9cf183acbd4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9aeb686b-f1b1-4cea-a1d4-96f36db135cf","metadata":{"id":"9aeb686b-f1b1-4cea-a1d4-96f36db135cf"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a7e4a4a9-ead2-418b-8528-b52b1b2eaba2","metadata":{"id":"a7e4a4a9-ead2-418b-8528-b52b1b2eaba2"},"source":["### DenseNET Model 169"]},{"cell_type":"code","execution_count":null,"id":"416030aa-01ee-49ee-9f30-f5ebba10c763","metadata":{"id":"416030aa-01ee-49ee-9f30-f5ebba10c763","outputId":"06b88120-62c0-4198-ca24-0f7168d40322"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n","C:\\Users\\tan_l\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to C:\\Users\\tan_l/.cache\\torch\\hub\\checkpoints\\densenet169-b2777c0a.pth\n","100%|██████████| 54.7M/54.7M [00:01<00:00, 41.2MB/s]\n"]}],"source":["import torch\n","\n","# input_batch = torch.stack(train_tensor_list)\n","# print(input_batch)\n","\n","# device = (\n","#     \"cuda\"\n","#     if torch.cuda.is_available()\n","#     else \"mps\"\n","#     if torch.backends.mps.is_available()\n","#     else \"cpu\"\n","# )\n","\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f\"Using {device} device\")\n","\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","# # or any of these variants\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)\n","# model.eval()\n","\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n","# or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n","model.eval()\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","\n","# with torch.no_grad():\n","#     output = model(input_batch)\n","# # Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n","# print(output[0])\n","# # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n","# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n","# print(probabilities)"]},{"cell_type":"code","execution_count":null,"id":"a4196854-923a-47b5-a6c1-3edcdc4fed57","metadata":{"id":"a4196854-923a-47b5-a6c1-3edcdc4fed57","outputId":"54127996-ae84-475b-b627-60a4d6ecf37a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-29 02:52:03.553995\n","Learning Rate: 0.01\n","Start Time : 1714330323.553995\n","End Time : 1714337113.6740913\n","Time Taken: 6790.120096445084\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 0.34774726927280425\n","Test Accuracy : 85.5\n","\n","-------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-29 04:45:13.921148\n","Learning Rate: 0.001\n","Start Time : 1714337113.921148\n","End Time : 1714343964.4019084\n","Time Taken: 6850.480760335922\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 0.48998236656188965\n","Test Accuracy : 78.66666666666666\n","\n","-------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-29 06:39:24.651007\n","Learning Rate: 0.0001\n","Start Time : 1714343964.6510074\n","End Time : 1714350713.1347501\n","Time Taken: 6748.483742713928\n","Epoch 10\n","-------------------------------\n","Train Loss : tensor(4.4934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Test Avg Loss : 4.883101892471314\n","Test Accuracy : 11.333333333333332\n","\n","-------------------------------\n","Done!\n"]}],"source":["optimizer_lr =[1e-2,1e-3,1e-4]\n","\n","for j in optimizer_lr:\n","    model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n","    model.train()\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=j)\n","\n","    def train(dataloader, model, loss_fn, optimizer):\n","        size = len(dataloader.dataset)\n","        model.train()\n","        for batch, (X, y) in enumerate(dataloader):\n","            X, y = X.to(device), y.to(device)\n","\n","            # Compute prediction error\n","            pred = model(X)\n","            loss = loss_fn(pred, y)\n","\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            if batch % 100 == 0:\n","                loss, current = loss.item(), (batch + 1) * len(X)\n","                # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","        return loss\n","\n","    def test(dataloader, model, loss_fn):\n","        size = len(dataloader.dataset)\n","        num_batches = len(dataloader)\n","        model.eval()\n","        test_loss, correct = 0, 0\n","        with torch.no_grad():\n","            for X, y in dataloader:\n","                X, y = X.to(device), y.to(device)\n","                pred = model(X)\n","                test_loss += loss_fn(pred, y).item()\n","                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","        test_loss /= num_batches\n","        correct /= size\n","        # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","        return (100*correct) , test_loss\n","\n","    epochs = 10\n","    print(\"Started:\", datetime.now())\n","\n","    start = time.time()\n","\n","    for t in range(epochs):\n","\n","        loss = train(trainloader, model, loss_fn, optimizer)\n","        accuracy , avg_loss = test(testloader, model, loss_fn)\n","        if t == (epochs-1):\n","            end = time.time()\n","            print(\"Learning Rate:\", j)\n","            print(\"Start Time :\", start)\n","            print(\"End Time :\", end)\n","            print(\"Time Taken:\", end - start)\n","            print(f\"Epoch {t+1}\\n-------------------------------\")\n","            print(\"Train Loss :\", loss)\n","            print(\"Test Avg Loss :\", avg_loss)\n","            print(\"Test Accuracy :\", accuracy)\n","            print(\"\\n-------------------------------\")\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"id":"2712ab44-1a64-4d77-828f-eaf962bd69fe","metadata":{"id":"2712ab44-1a64-4d77-828f-eaf962bd69fe"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ac2a79f4-69c3-4438-90c6-588c304117c6","metadata":{"id":"ac2a79f4-69c3-4438-90c6-588c304117c6"},"source":["### DenseNET Model 201"]},{"cell_type":"code","execution_count":null,"id":"d9863449-54b5-423e-aa4a-24cdb108ef9c","metadata":{"id":"d9863449-54b5-423e-aa4a-24cdb108ef9c","outputId":"26f3c45c-650f-40b3-8c1e-6630f78b82cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]},{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n","C:\\Users\\tan_l\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to C:\\Users\\tan_l/.cache\\torch\\hub\\checkpoints\\densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:00<00:00, 94.6MB/s]\n"]}],"source":["import torch\n","\n","# input_batch = torch.stack(train_tensor_list)\n","# print(input_batch)\n","\n","# device = (\n","#     \"cuda\"\n","#     if torch.cuda.is_available()\n","#     else \"mps\"\n","#     if torch.backends.mps.is_available()\n","#     else \"cpu\"\n","# )\n","\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f\"Using {device} device\")\n","\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","# # or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n","# # model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)\n","# model.eval()\n","\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n","# or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n","model.eval()\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","\n","# with torch.no_grad():\n","#     output = model(input_batch)\n","# # Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n","# print(output[0])\n","# # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n","# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n","# print(probabilities)"]},{"cell_type":"code","execution_count":null,"id":"c9524e28-864d-4e41-aa50-2e188aa73599","metadata":{"id":"c9524e28-864d-4e41-aa50-2e188aa73599","outputId":"aaed55ea-31eb-4658-9807-4bef3cd9b6a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\tan_l/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["Started: 2024-04-29 09:37:29.297187\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.03 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(trainloader, model, loss_fn, optimizer)\n\u001b[0;32m     57\u001b[0m     accuracy , avg_loss \u001b[38;5;241m=\u001b[39m test(testloader, model, loss_fn)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m (epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n","Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\densenet.py:213\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 213\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m    214\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n","File \u001b[1;32m~\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.03 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["optimizer_lr =[1e-2,1e-3,1e-4]\n","\n","for j in optimizer_lr:\n","    model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n","    model.train()\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=j)\n","\n","    def train(dataloader, model, loss_fn, optimizer):\n","        size = len(dataloader.dataset)\n","        model.train()\n","        for batch, (X, y) in enumerate(dataloader):\n","            X, y = X.to(device), y.to(device)\n","\n","            # Compute prediction error\n","            pred = model(X)\n","            loss = loss_fn(pred, y)\n","\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            if batch % 100 == 0:\n","                loss, current = loss.item(), (batch + 1) * len(X)\n","                # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","        return loss\n","\n","    def test(dataloader, model, loss_fn):\n","        size = len(dataloader.dataset)\n","        num_batches = len(dataloader)\n","        model.eval()\n","        test_loss, correct = 0, 0\n","        with torch.no_grad():\n","            for X, y in dataloader:\n","                X, y = X.to(device), y.to(device)\n","                pred = model(X)\n","                test_loss += loss_fn(pred, y).item()\n","                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","        test_loss /= num_batches\n","        correct /= size\n","        # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","        return (100*correct) , test_loss\n","\n","    epochs = 10\n","    print(\"Started:\", datetime.now())\n","\n","    start = time.time()\n","\n","    for t in range(epochs):\n","\n","        loss = train(trainloader, model, loss_fn, optimizer)\n","        accuracy , avg_loss = test(testloader, model, loss_fn)\n","        if t == (epochs-1):\n","            end = time.time()\n","            print(\"Learning Rate:\", j)\n","            print(\"Start Time :\", start)\n","            print(\"End Time :\", end)\n","            print(\"Time Taken:\", end - start)\n","            print(f\"Epoch {t+1}\\n-------------------------------\")\n","            print(\"Train Loss :\", loss)\n","            print(\"Test Avg Loss :\", avg_loss)\n","            print(\"Test Accuracy :\", accuracy)\n","            print(\"\\n-------------------------------\")\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"id":"d7dd521a-ae51-43d8-a989-58c40c89ae52","metadata":{"id":"d7dd521a-ae51-43d8-a989-58c40c89ae52"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ae3a0e02-72ec-4244-9326-ec8355d4ab6d","metadata":{"id":"ae3a0e02-72ec-4244-9326-ec8355d4ab6d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0de8c8c5-3123-46d2-bf83-418bc1080804","metadata":{"id":"0de8c8c5-3123-46d2-bf83-418bc1080804","outputId":"50bee58e-06f0-42a1-c1d6-d52093fc13ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Started: 2024-04-08 02:52:52.648596\n","Epoch 1\n","-------------------------------\n","loss: 16.575052  [  128/33000]\n","loss: 0.512749  [12928/33000]\n","loss: 0.468490  [25728/33000]\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.374671 \n","\n","Epoch 2\n","-------------------------------\n","loss: 0.383459  [  128/33000]\n","loss: 0.224207  [12928/33000]\n","loss: 0.223989  [25728/33000]\n","Test Error: \n"," Accuracy: 88.9%, Avg loss: 0.270216 \n","\n","Epoch 3\n","-------------------------------\n","loss: 0.234675  [  128/33000]\n","loss: 0.234386  [12928/33000]\n","loss: 0.214994  [25728/33000]\n","Test Error: \n"," Accuracy: 91.1%, Avg loss: 0.221622 \n","\n","Epoch 4\n","-------------------------------\n","loss: 0.188882  [  128/33000]\n","loss: 0.222569  [12928/33000]\n","loss: 0.192542  [25728/33000]\n","Test Error: \n"," Accuracy: 92.3%, Avg loss: 0.194899 \n","\n","Epoch 5\n","-------------------------------\n","loss: 0.159839  [  128/33000]\n","loss: 0.164312  [12928/33000]\n","loss: 0.123608  [25728/33000]\n","Test Error: \n"," Accuracy: 93.0%, Avg loss: 0.177767 \n","\n","Epoch 6\n","-------------------------------\n","loss: 0.181024  [  128/33000]\n","loss: 0.121823  [12928/33000]\n","loss: 0.127180  [25728/33000]\n","Test Error: \n"," Accuracy: 93.7%, Avg loss: 0.163269 \n","\n","Epoch 7\n","-------------------------------\n","loss: 0.117868  [  128/33000]\n","loss: 0.083122  [12928/33000]\n","loss: 0.126456  [25728/33000]\n","Test Error: \n"," Accuracy: 94.1%, Avg loss: 0.152642 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.121343  [  128/33000]\n","loss: 0.122596  [12928/33000]\n","loss: 0.186412  [25728/33000]\n","Test Error: \n"," Accuracy: 94.5%, Avg loss: 0.143959 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.084883  [  128/33000]\n","loss: 0.090886  [12928/33000]\n","loss: 0.069759  [25728/33000]\n","Test Error: \n"," Accuracy: 94.8%, Avg loss: 0.137590 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.082153  [  128/33000]\n","loss: 0.090425  [12928/33000]\n","loss: 0.104112  [25728/33000]\n","Test Error: \n"," Accuracy: 95.0%, Avg loss: 0.132525 \n","\n","Done!\n"]}],"source":["# epochs = 10\n","# print(\"Started:\", datetime.now())\n","# for t in range(epochs):\n","#     print(f\"Epoch {t+1}\\n-------------------------------\")\n","#     train(trainloader, model, loss_fn, optimizer)\n","#     test(testloader, model, loss_fn)\n","# print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"id":"14470e44-b8a2-409f-833f-0778a2b9c353","metadata":{"id":"14470e44-b8a2-409f-833f-0778a2b9c353","outputId":"1ea41d5c-eb0d-48c5-a2fc-2c9f9c137446"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished: 2024-04-08 18:30:15.324487\n","Saved PyTorch Model State to model.pth\n"]}],"source":["# # Save model\n","\n","# print(\"Finished:\", datetime.now())\n","# torch.save(model.state_dict(), \"model.pth\")\n","# print(\"Saved PyTorch Model State to model.pth\")"]},{"cell_type":"code","execution_count":null,"id":"4e5b74b6-6dad-4f03-9709-01362aa6f536","metadata":{"id":"4e5b74b6-6dad-4f03-9709-01362aa6f536"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}