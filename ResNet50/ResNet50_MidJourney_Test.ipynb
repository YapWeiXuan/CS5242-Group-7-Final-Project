{"cells":[{"cell_type":"code","execution_count":null,"id":"8c8f19c6-6e2d-42c7-ac9a-4eb657b7666a","metadata":{"id":"8c8f19c6-6e2d-42c7-ac9a-4eb657b7666a"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from datetime import datetime\n","\n","import PIL"]},{"cell_type":"code","execution_count":null,"id":"c499460b-a609-4bee-9dc6-d216951d24aa","metadata":{"id":"c499460b-a609-4bee-9dc6-d216951d24aa","outputId":"17531e8b-65b5-4f61-b147-0788af26880c"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torchvision\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"id":"1aa8a13c-d950-4659-ab28-5456c9a2fdaa","metadata":{"id":"1aa8a13c-d950-4659-ab28-5456c9a2fdaa"},"outputs":[],"source":["import os\n","from os import listdir"]},{"cell_type":"code","execution_count":null,"id":"86af81c6-5d17-4dd6-86af-8cd07cc79f2a","metadata":{"id":"86af81c6-5d17-4dd6-86af-8cd07cc79f2a"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"id":"9159c84c-2be9-4100-ab94-7c82d7e7bcc6","metadata":{"id":"9159c84c-2be9-4100-ab94-7c82d7e7bcc6"},"outputs":[],"source":["def ignore_alpha(image):\n","    if image.mode == 'RGBA':\n","        r, g, b, _ = image.split()\n","        return PIL.Image.merge(\"RGB\", (r, g, b))\n","    elif image.mode == 'RGB':\n","        return image\n","    else:\n","        return image.convert(\"RGB\")\n","\n","transform_train_V1 = transforms.Compose([\n","    ignore_alpha,\n","    transforms.Resize((224,224)),\n","    transforms.CenterCrop((224,224)),\n","    transforms.RandomHorizontalFlip(), # To introduce image orientation variation\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","])\n","\n","transform_test_V1 = transforms.Compose([\n","    ignore_alpha,\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])"]},{"cell_type":"code","execution_count":null,"id":"40eb430a-444b-4dbf-90c9-5fb53c85e9c1","metadata":{"id":"40eb430a-444b-4dbf-90c9-5fb53c85e9c1"},"outputs":[],"source":["# Folders\n","train_FAKE_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_subset/FAKE\"\n","train_REAL_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_subset/REAL\"\n","\n","test_FAKE_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/ai\"\n","test_REAL_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/nature\""]},{"cell_type":"markdown","id":"8b0c4676-8a5c-4fd4-9cd4-7ecb278b9798","metadata":{"id":"8b0c4676-8a5c-4fd4-9cd4-7ecb278b9798"},"source":["### MidJourney Test Folder Path"]},{"cell_type":"code","execution_count":null,"id":"1baa9a1b-ed36-4795-bf20-d8f373b64e69","metadata":{"id":"1baa9a1b-ed36-4795-bf20-d8f373b64e69"},"outputs":[],"source":["MJ_test_FAKE_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/ai\"\n","MJ_test_REAL_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/nature\""]},{"cell_type":"code","execution_count":null,"id":"4a13b10c-25d2-4c85-8375-bd398aa903e6","metadata":{"id":"4a13b10c-25d2-4c85-8375-bd398aa903e6"},"outputs":[],"source":["MJ_test_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(MJ_test_FAKE_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{MJ_test_FAKE_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    MJ_test_tensor_list.append((transform_test_V1(img),0))\n","# REAL portion\n","\n","for images in os.listdir(MJ_test_REAL_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{MJ_test_REAL_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    MJ_test_tensor_list.append((transform_test_V1(img),1))"]},{"cell_type":"code","execution_count":null,"id":"17f55cc5-1d2a-4680-bb33-c7bfaef221ab","metadata":{"id":"17f55cc5-1d2a-4680-bb33-c7bfaef221ab","outputId":"86ef5e3d-b021-448e-aceb-e990c5464044"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, H, W]: torch.Size([128, 3, 224, 224])\n","Shape of y: torch.Size([128]) torch.int64\n"]}],"source":["MJ_testloader = torch.utils.data.DataLoader(MJ_test_tensor_list, batch_size=128,shuffle=False, num_workers=2)\n","\n","for X, y in MJ_testloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","id":"fda1e594-18fa-43b9-a970-36d2f968d2fa","metadata":{"id":"fda1e594-18fa-43b9-a970-36d2f968d2fa"},"source":["### SD Test Folder Path"]},{"cell_type":"code","execution_count":null,"id":"d3aee388-0c00-4ab7-8b1a-d13b91b38499","metadata":{"id":"d3aee388-0c00-4ab7-8b1a-d13b91b38499"},"outputs":[],"source":["SD_test_FAKE_folder_path = \"C:/Users/flame/Documents/imagenet_ai_0419_sdv4_val/val/ai\"\n","SD_test_REAL_folder_path = \"C:/Users/flame/Documents/imagenet_ai_0419_sdv4_val/val/nature\""]},{"cell_type":"code","execution_count":null,"id":"20c04d1e-fa22-49f9-be53-6241e558465f","metadata":{"id":"20c04d1e-fa22-49f9-be53-6241e558465f"},"outputs":[],"source":["SD_test_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(SD_test_FAKE_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{SD_test_FAKE_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    SD_test_tensor_list.append((transform_test_V1(img),0))\n","# REAL portion\n","\n","for images in os.listdir(SD_test_REAL_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{SD_test_REAL_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    SD_test_tensor_list.append((transform_test_V1(img),1))"]},{"cell_type":"code","execution_count":null,"id":"8738a160-3109-4dba-8ba4-169187136658","metadata":{"id":"8738a160-3109-4dba-8ba4-169187136658","outputId":"0089eb1e-0f23-42f3-91b1-ea4992b4f260"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, H, W]: torch.Size([128, 3, 224, 224])\n","Shape of y: torch.Size([128]) torch.int64\n"]}],"source":["SD_testloader = torch.utils.data.DataLoader(SD_test_tensor_list, batch_size=128,shuffle=False, num_workers=2)\n","\n","for X, y in SD_testloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","id":"93fe81c2-8251-4c59-8b9a-8df3b016461f","metadata":{"id":"93fe81c2-8251-4c59-8b9a-8df3b016461f"},"source":["### Big Gan Test Folder Path"]},{"cell_type":"code","execution_count":null,"id":"cc1f19d1-69e0-4bc8-95f2-9d07b107ac13","metadata":{"id":"cc1f19d1-69e0-4bc8-95f2-9d07b107ac13"},"outputs":[],"source":["BG_test_FAKE_folder_path = \"C:/Users/flame/Documents/imagenet_ai_0419_biggan_val/val/ai\"\n","BG_test_REAL_folder_path = \"C:/Users/flame/Documents/imagenet_ai_0419_biggan_val/val/nature\""]},{"cell_type":"code","execution_count":null,"id":"0d842b64-d777-4ca7-a140-bfc1ddca9c68","metadata":{"id":"0d842b64-d777-4ca7-a140-bfc1ddca9c68"},"outputs":[],"source":["BG_test_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(BG_test_FAKE_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{BG_test_FAKE_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    BG_test_tensor_list.append((transform_test_V1(img),0))\n","# REAL portion\n","\n","for images in os.listdir(BG_test_REAL_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{BG_test_REAL_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    BG_test_tensor_list.append((transform_test_V1(img),1))"]},{"cell_type":"code","execution_count":null,"id":"2230ffc0-85d6-4238-a179-a5a9bcc1225d","metadata":{"id":"2230ffc0-85d6-4238-a179-a5a9bcc1225d","outputId":"a0a6ad28-9429-4011-98fb-f8fa70d47708"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, H, W]: torch.Size([128, 3, 224, 224])\n","Shape of y: torch.Size([128]) torch.int64\n"]}],"source":["BG_testloader = torch.utils.data.DataLoader(BG_test_tensor_list, batch_size=128,shuffle=False, num_workers=2)\n","\n","for X, y in BG_testloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","id":"70e9cfdb-0c81-41b6-b216-d9f8fe7253a3","metadata":{"id":"70e9cfdb-0c81-41b6-b216-d9f8fe7253a3"},"source":["### Load Model"]},{"cell_type":"code","execution_count":null,"id":"b8006830-a367-466d-a2d0-aa4144cc6f84","metadata":{"id":"b8006830-a367-466d-a2d0-aa4144cc6f84","outputId":"e7d07e56-6b1c-450d-f2e9-33ff81487f5e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\flame/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n","C:\\Users\\flame\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","C:\\Users\\flame\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["base_model_test = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","path = \"ResNet50-MidJourney-Big-DataSet-model.pth\"\n","\n","base_model_test.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"id":"61f65788-161a-451e-887c-ac114f868206","metadata":{"id":"61f65788-161a-451e-887c-ac114f868206"},"outputs":[],"source":["def final_evaluation(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    pred_list =[]\n","    true_list=[]\n","\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","\n","            true_list.extend(y.cpu().tolist())\n","            pred_list.extend(pred.argmax(1).cpu().tolist())\n","\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","\n","    # f1_result = f1_score(true_list, pred_list, average='micro')\n","    # tn, fp, fn, tp = confusion_matrix(true_list, pred_list).ravel()\n","    # confusion_accuracy = (tp+tn)/(tn+fp+fn+tp)\n","    # confusion_recall = tp/(tp+fn)\n","    # confusion_precision = tp/(tp+fp)\n","\n","    # print(f\"Confusion Matrix: \\n Accuracy: {(100*confusion_accuracy):>0.1f}% \\n Recall: {(100*confusion_recall):>0.1f}% \\n Precision: {(100*confusion_precision):>0.1f}% \\n\")\n","    # print(f\"F1 Result: {f1_result} \\n\")\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":null,"id":"ead3caa6-519b-44e0-a155-e2cd9815fbf4","metadata":{"id":"ead3caa6-519b-44e0-a155-e2cd9815fbf4"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","if torch.cuda.is_available():\n","    base_model_test.cuda()"]},{"cell_type":"code","execution_count":null,"id":"e7d8b469-95bd-47bf-a5f1-21bf347d1195","metadata":{"id":"e7d8b469-95bd-47bf-a5f1-21bf347d1195","outputId":"f2aa9c03-d407-4719-917d-9ae82f0f65db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix: \n"," Accuracy: 94.8% \n"," Recall: 94.4% \n"," Precision: 95.1% \n","\n","F1 Result: 0.9479166666666666 \n","\n","Test Error: \n"," Accuracy: 94.8%, Avg loss: 0.135091 \n","\n"]}],"source":["final_evaluation(MJ_testloader, base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"fb02877e-ae49-43ea-ab64-502157cf9f89","metadata":{"id":"fb02877e-ae49-43ea-ab64-502157cf9f89","outputId":"0f61bc33-94e7-4ec3-ec65-4ee108fe573f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 94.8%, Avg loss: 0.135091 \n","\n"]}],"source":["final_evaluation(MJ_testloader, base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"4e3b0f9a-21ea-47c3-b504-b4ec202f704f","metadata":{"id":"4e3b0f9a-21ea-47c3-b504-b4ec202f704f","outputId":"791e226b-542a-44b5-8c70-6ac47dd63f6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 48.9%, Avg loss: 3.078392 \n","\n"]}],"source":["final_evaluation(BG_testloader, base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"2a81cb1e-17f8-46e7-aa97-7e6648035a22","metadata":{"id":"2a81cb1e-17f8-46e7-aa97-7e6648035a22","outputId":"faf374a2-a6f7-4ef6-a757-871a99045d16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 73.2%, Avg loss: 0.808466 \n","\n"]}],"source":["final_evaluation(SD_testloader, base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"88d0d3f3-d9c9-4181-b0a5-d74724e9af42","metadata":{"id":"88d0d3f3-d9c9-4181-b0a5-d74724e9af42"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"595621ac-fec4-4ba2-bae6-3cc9f71f4a6e","metadata":{"id":"595621ac-fec4-4ba2-bae6-3cc9f71f4a6e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"38a97973-efeb-4894-820d-3ef584c5323c","metadata":{"id":"38a97973-efeb-4894-820d-3ef584c5323c","outputId":"d9622b14-c991-4296-8565-982186078799"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\flame/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n","C:\\Users\\flame\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","C:\\Users\\flame\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["SD_base_model_test = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","path = \"resnet-sdv4-model.pth\"\n","\n","SD_base_model_test.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"id":"9619fa20-507a-4ebc-965a-6533feb854a8","metadata":{"id":"9619fa20-507a-4ebc-965a-6533feb854a8"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","if torch.cuda.is_available():\n","    SD_base_model_test.cuda()"]},{"cell_type":"code","execution_count":null,"id":"e550e1e2-b5ab-4190-86b1-6f68e85a28b5","metadata":{"id":"e550e1e2-b5ab-4190-86b1-6f68e85a28b5","outputId":"7fdd8121-bb63-4b21-825f-b8147066e9d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 66.1%, Avg loss: 1.745989 \n","\n"]}],"source":["final_evaluation(MJ_testloader, SD_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"04e99ede-6c8b-4cb9-a0f0-0c3eed467a54","metadata":{"id":"04e99ede-6c8b-4cb9-a0f0-0c3eed467a54","outputId":"8ed5c29d-1b97-4b58-d528-465a15eecffe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 49.2%, Avg loss: 3.569276 \n","\n"]}],"source":["final_evaluation(BG_testloader, SD_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"a0506a65-085c-493e-956c-a38d55893495","metadata":{"id":"a0506a65-085c-493e-956c-a38d55893495","outputId":"3087e6d7-294f-4120-dcfd-1197fe63f85e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 95.8%, Avg loss: 0.111360 \n","\n"]}],"source":["final_evaluation(SD_testloader, SD_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"d5df6343-0cf1-4768-adc9-5c70defd30ef","metadata":{"id":"d5df6343-0cf1-4768-adc9-5c70defd30ef"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e1fd0a09-973c-45cf-956f-25fc8c9e2d49","metadata":{"id":"e1fd0a09-973c-45cf-956f-25fc8c9e2d49"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"875e8219-3ee5-4053-a11d-c919d6e83f0e","metadata":{"id":"875e8219-3ee5-4053-a11d-c919d6e83f0e","outputId":"df1db64a-9d39-47ab-d886-45530128108b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\flame/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["BG_base_model_test = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","path = \"resnet-biggan-model.pth\"\n","\n","BG_base_model_test.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"id":"1b450b33-d286-4251-9110-525b1f7976ee","metadata":{"id":"1b450b33-d286-4251-9110-525b1f7976ee"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","if torch.cuda.is_available():\n","    BG_base_model_test.cuda()"]},{"cell_type":"code","execution_count":null,"id":"c6591e92-f33c-47e8-9a6e-eaa50a6c905c","metadata":{"id":"c6591e92-f33c-47e8-9a6e-eaa50a6c905c","outputId":"4e6d659a-b2b0-46f7-962d-ebaba9ee7917"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 50.1%, Avg loss: 5.119463 \n","\n"]}],"source":["final_evaluation(MJ_testloader, BG_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"3d524baa-d655-41a5-822f-61c67baf947d","metadata":{"id":"3d524baa-d655-41a5-822f-61c67baf947d","outputId":"d6ab8a25-ee2c-4496-b628-159771d9b6e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 99.7%, Avg loss: 0.012620 \n","\n"]}],"source":["final_evaluation(BG_testloader, BG_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"15a18a11-6005-4299-b1c8-e36222639c4a","metadata":{"id":"15a18a11-6005-4299-b1c8-e36222639c4a","outputId":"d9c7b8ec-0c93-4199-99e5-466820272d44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 49.8%, Avg loss: 6.043572 \n","\n"]}],"source":["final_evaluation(SD_testloader, BG_base_model_test, loss_fn)"]},{"cell_type":"code","execution_count":null,"id":"90815228-1c86-4aa4-9133-d35e750dd4dd","metadata":{"id":"90815228-1c86-4aa4-9133-d35e750dd4dd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"753b0f59-0a34-4416-a2cd-d45b0868cd90","metadata":{"id":"753b0f59-0a34-4416-a2cd-d45b0868cd90"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ffcbbfaf-7307-4096-aa4e-e4a9c07b394a","metadata":{"id":"ffcbbfaf-7307-4096-aa4e-e4a9c07b394a","outputId":"8819aa7c-c8d4-42a2-87da-9ae77e417119"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\flame/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["DENSENET_SD_base_model_test = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","path = \"C:/Users/flame/Documents/CS5424/Densenet models/sd-v4-run-2-model.pth\"\n","\n","DENSENET_SD_base_model_test.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"id":"046ebea3-a53e-473f-8971-8edfaa31e578","metadata":{"id":"046ebea3-a53e-473f-8971-8edfaa31e578"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","if torch.cuda.is_available():\n","    DENSENET_SD_base_model_test.cuda()"]},{"cell_type":"code","execution_count":null,"id":"9e96d4df-d335-4a81-aa64-23445282bd33","metadata":{"id":"9e96d4df-d335-4a81-aa64-23445282bd33","outputId":"876cc1df-8881-441f-e2d7-412e44a58608"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Error: \n"," Accuracy: 63.7%, Avg loss: 1.774629 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 2.914268 \n","\n","Test Error: \n"," Accuracy: 95.0%, Avg loss: 0.132525 \n","\n","MJ test accuracy : None\n","BJ test accuracy : None\n","SD test accuracy : None\n"]}],"source":["MJ_TEST = final_evaluation(MJ_testloader, DENSENET_SD_base_model_test, loss_fn)\n","BG_TEST = final_evaluation(BG_testloader, DENSENET_SD_base_model_test, loss_fn)\n","SD_TEST = final_evaluation(SD_testloader, DENSENET_SD_base_model_test, loss_fn)\n","\n","print(\"MJ test accuracy :\", MJ_TEST)\n","print(\"BJ test accuracy :\", BG_TEST)\n","print(\"SD test accuracy :\", SD_TEST)"]},{"cell_type":"code","execution_count":null,"id":"eb9167ab-853e-4714-9f33-126c67ecf02c","metadata":{"id":"eb9167ab-853e-4714-9f33-126c67ecf02c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c0580c21-49f5-44c7-9554-dba6e61c5673","metadata":{"id":"c0580c21-49f5-44c7-9554-dba6e61c5673"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1db6d46c-06de-4967-8cdc-5af6b6aac078","metadata":{"id":"1db6d46c-06de-4967-8cdc-5af6b6aac078"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0e84a458-3d5b-4f75-bc5c-e0f8370bc9e5","metadata":{"id":"0e84a458-3d5b-4f75-bc5c-e0f8370bc9e5"},"outputs":[],"source":["train_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(train_FAKE_folder_path):\n","    # print(images)\n","    # if images.endswith(\".png\"):\n","    img = PIL.Image.open(f\"{train_FAKE_folder_path}/%s\" % images)\n","    # train_tensor_list.append(transform_train_V1(img))\n","    train_tensor_list.append((transform_train_V1(img), 0))\n","# REAL portion\n","for images in os.listdir(train_REAL_folder_path):\n","    # if images.endswith(\".png\"):\n","    img = PIL.Image.open(f\"{train_REAL_folder_path}/%s\" % images)\n","    # train_tensor_list.append(transform_train_V1(img))\n","    train_tensor_list.append((transform_train_V1(img), 1))"]},{"cell_type":"code","execution_count":null,"id":"290e99e3-66f1-4c1d-bfa4-4c3f7d6ff060","metadata":{"id":"290e99e3-66f1-4c1d-bfa4-4c3f7d6ff060","outputId":"b1a14503-05ef-420b-befb-e53768785a95"},"outputs":[{"ename":"RuntimeError","evalue":"[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 602112 bytes.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_FAKE_folder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/%s\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m images)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# test_tensor_list.append(transform_test_V1(img))\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     test_tensor_list\u001b[38;5;241m.\u001b[39mappend((\u001b[43mtransform_test_V1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# REAL portion\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(test_REAL_folder_path):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# print(images)\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:175\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 602112 bytes."]}],"source":["test_tensor_list =[]\n","#Format = list of tuples with each tuple in this format ((tensor version of image), label)\n","\n","# FAKE portion\n","for images in os.listdir(test_FAKE_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{test_FAKE_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    test_tensor_list.append((transform_test_V1(img),0))\n","# REAL portion\n","\n","for images in os.listdir(test_REAL_folder_path):\n","    # print(images)\n","    img = PIL.Image.open(f\"{test_REAL_folder_path}/%s\" % images)\n","    # test_tensor_list.append(transform_test_V1(img))\n","    test_tensor_list.append((transform_test_V1(img),1))"]},{"cell_type":"code","execution_count":null,"id":"a8611b3a-8927-4ef4-8b52-f7ed6272aa97","metadata":{"id":"a8611b3a-8927-4ef4-8b52-f7ed6272aa97"},"outputs":[],"source":["print(len(train_tensor_list))\n","print(len(test_tensor_list))"]},{"cell_type":"code","execution_count":null,"id":"a051cd01-a1fa-4c66-b313-b1858b8610d1","metadata":{"id":"a051cd01-a1fa-4c66-b313-b1858b8610d1"},"outputs":[],"source":["print(len(test_tensor_list))"]},{"cell_type":"code","execution_count":null,"id":"8365671f-d912-4a3c-9c90-d890719db245","metadata":{"id":"8365671f-d912-4a3c-9c90-d890719db245"},"outputs":[],"source":["# Folders\n","train_FAKE_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_subset/FAKE\"\n","train_REAL_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_subset/REAL\"\n","\n","test_FAKE_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/ai\"\n","test_REAL_folder_path = \"C:/Users/flame/Documents/MidJourney Training/imagenet_midjourney_test/imagenet_midjourney_val/val/nature\""]},{"cell_type":"code","execution_count":null,"id":"49d0fae8-42f9-4251-a807-b2f2d03c4681","metadata":{"id":"49d0fae8-42f9-4251-a807-b2f2d03c4681","outputId":"adc2ec70-3d97-49fe-fe00-28644c5b952d"},"outputs":[{"ename":"AttributeError","evalue":"'collections.OrderedDict' object has no attribute 'eval'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(PATH)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = TheModelClass(*args, **kwargs)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load(PATH))\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n","\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"]}],"source":["PATH = \"ResNet50-MidJourney-Big-DataSet-model.pth\"\n","model = torch.load(PATH)\n","# model = TheModelClass(*args, **kwargs)\n","# model.load_state_dict(torch.load(PATH))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"id":"9a6e48cc-9b8a-499b-96a0-2586210868bb","metadata":{"id":"9a6e48cc-9b8a-499b-96a0-2586210868bb","outputId":"6593341b-47e7-441a-909d-96b8c272e952"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in C:\\Users\\flame/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"]}],"source":["model_test = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n"]},{"cell_type":"code","execution_count":null,"id":"2b27cf2f-c389-49b2-8208-178b7ba23f86","metadata":{"id":"2b27cf2f-c389-49b2-8208-178b7ba23f86"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"id":"529f27c9-a51b-458c-8953-4aa7b8311707","metadata":{"id":"529f27c9-a51b-458c-8953-4aa7b8311707"},"outputs":[],"source":["def final_evaluation(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    pred_list =[]\n","    true_list=[]\n","\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","\n","            true_list.append(y)\n","            pred_list.append(pred)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":null,"id":"5482f837-c2ce-49ad-a12e-c3c717130f9f","metadata":{"id":"5482f837-c2ce-49ad-a12e-c3c717130f9f"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}